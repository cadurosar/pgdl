# Copyright 2020 The PGDL Competition organizers.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Utilities for loading models for PGDL competition at NeurIPS 2020
# Main contributor: Yiding Jiang, July 2020

# This complexity compute a specific notion of sharpness of a function.


import numpy as np
import tensorflow as tf

def complexity(model, ds):

  @tf.function
  def compute_bounds(weights, orig_weights, mag):
    max_perturb = [tf.abs(v) * mag for v in orig_weights]
    upper_bound = [v + vp for v, vp in zip(orig_weights, max_perturb)]
    lower_bound = [v - vp for v, vp in zip(orig_weights, max_perturb)]
    return upper_bound, lower_bound

  @tf.function
  def compute_clipped_weights(weights, upper_bound, lower_bound):
    clipped_weights = []
    for i, v in enumerate(weights):
      clipped_v = tf.minimum(tf.maximum(v, lower_bound[i]), upper_bound[i])
      clipped_weights.append(clipped_v)
    return clipped_weights

  @tf.function
  def optimize(loss_fn, optimizer, data):
    x, y = data
    # y = tf.one_hot(y, 10)
    with tf.GradientTape() as tape:
      logits = model(x)
      loss = loss_fn(y, logits)
    variables = model.trainable_variables
    gradients = tape.gradient(loss, variables)
    optimizer.apply_gradients(zip(gradients, variables))

  @tf.function
  def predict(x):
    logits = model(x)
    pred = tf.cast(tf.argmax(logits, axis=-1), tf.int32)
    return pred

  def get_noise(weights, scale):
    uniform_noise = []
    for w in weights:
      uniform_noise.append(np.random.uniform(low=-scale/2, high=scale/2, size=list(w.shape)))
      uniform_noise[-1] *= np.abs(w)
    return uniform_noise

  def estimate_accuracy(model, dataset, iteration):
    acc = 0.0
    for i in range(iteration):
      x, y = dataset.next()
      pred = predict(x).numpy()
      acc += np.mean(pred == y)
    return acc / iteration

  batched_ds = iter(ds.shuffle(1000).repeat(-1).batch(64))
  orig_weights = model.get_weights()
  cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
  optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)

  h, l = 2.0, 0.000000
  target_deviate = 0.1
  epsilon = 1e-8
  target_accuracy = 0.9
  outer_n = 10
  for i in range(outer_n):
    m = (h + l) / 2.
    min_accuracy = 10.0
    for k in range(3):
      if min_accuracy < 0.8: continue
      noise = get_noise(orig_weights, m)
      model.set_weights([w+n for w, n in zip(orig_weights, noise)])
      for j in range(20):
        data_pair = batched_ds.next()
        optimize(cce, optimizer, data_pair)
        curr_weights = [tf.convert_to_tensor(w) for w in model.get_weights()]
        upper, lower = compute_bounds(curr_weights, orig_weights, tf.convert_to_tensor(m))
        clipped_weights = compute_clipped_weights(curr_weights, upper, lower)
        model.set_weights(clipped_weights)
      estimate = []
      repeat = 3 if min_accuracy < 0.8 else 10
      estimate_mean = estimate_accuracy(model, batched_ds, repeat)
      min_accuracy = min(min_accuracy, estimate_mean)

    deviate = 1.0 - min_accuracy  # training accuracy - current accuracy
    if h - l < epsilon or abs(deviate - target_deviate) < 1e-2:
      break
    if deviate > target_deviate:
      h = m
    else:
      l = m
  return m

